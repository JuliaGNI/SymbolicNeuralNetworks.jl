var documenterSearchIndex = {"docs":
[{"location":"symbolic_neural_networks/#Symbolic-Neural-Networks","page":"Vanilla Symbolic Neural Network","title":"Symbolic Neural Networks","text":"","category":"section"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"When using a symbolic neural network we can use architectures from GeometricMachineLearning or more simple building blocks.","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"We first call the symbolic neural network that only consists of one layer:","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"using SymbolicNeuralNetworks\nusing AbstractNeuralNetworks: Chain, Dense, params\n\ninput_dim = 2\noutput_dim = 1\nhidden_dim = 3\nc = Chain(Dense(input_dim, hidden_dim), Dense(hidden_dim, hidden_dim), Dense(hidden_dim, output_dim))\nnn = SymbolicNeuralNetwork(c)\nnothing # hide","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"We can now build symbolic expressions based on this neural network. Here we do so by calling evaluate_equations:","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"using Symbolics\nusing Latexify: latexify\n\n@variables sinput[1:input_dim]\nsoutput = nn.model(sinput, params(nn))\n\nsoutput","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"or use Symbolics.scalarize to get a more readable version of the equation:","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"soutput |> Symbolics.scalarize","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"We can compute the symbolic gradient with SymbolicNeuralNetworks.Gradient:","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"using SymbolicNeuralNetworks: derivative\nderivative(SymbolicNeuralNetworks.Gradient(soutput, nn))[1].L1.b","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"info: Info\nSymbolicNeuralNetworks.Gradient can also be called as SymbolicNeuralNetworks.Gradient(snn), so without providing a specific output. In this case soutput is taken to be the symbolic output of the network (i.e. is equivalent to the construction presented here). Also note that we further called SymbolicNeuralNetworks.derivative here in order to get the symbolic gradient (as opposed to the symbolic output of the neural network).","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"In order to train a SymbolicNeuralNetwork we can use:","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"pb = SymbolicPullback(nn)\n\nnothing # hide","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"info: Info\nSymbolicNeuralNetworks.Gradient and SymbolicPullback both use the function SymbolicNeuralNetworks.symbolic_pullback internally, so are computationally equivalent. SymbolicPullback should however be used in connection to a NetworkLoss and SymbolicNeuralNetworks.Gradient can be used more generally to compute the derivative of array-valued expressions.","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"We want to use our one-layer neural network to approximate a Gaussian on the interval -1 1times-1 1. We fist generate the data for this task:","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"using GeometricMachineLearning\n\nx_vec = -1.:.1:1.\ny_vec = -1.:.1:1.\nxy_data = hcat([[x, y] for x in x_vec, y in y_vec]...)\nf(x::Vector) = exp.(-sum(x.^2))\nz_data = mapreduce(i -> f(xy_data[:, i]), hcat, axes(xy_data, 2))\n\ndl = DataLoader(xy_data, z_data)\nnothing # hide","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"Note that we use GeometricMachineLearning.DataLoader to process the data. We further also visualize them: ","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"using CairoMakie\n\nfig = Figure()\nax = Axis3(fig[1, 1])\nsurface!(x_vec, y_vec, [f([x, y]) for x in x_vec, y in y_vec]; alpha = .8, transparency = true)\nfig","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"We now train the network:","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"import Random # hide\nRandom.seed!(123) # hide\nnn_cpu = NeuralNetwork(c, CPU())\no = Optimizer(AdamOptimizer(), nn_cpu)\nn_epochs = 1000\nbatch = Batch(10)\no(nn_cpu, dl, batch, n_epochs, pb.loss, pb; show_progress = false); # hide\n@time o(nn_cpu, dl, batch, n_epochs, pb.loss, pb; show_progress = false);\nnothing # hide","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"We now compare the neural network-approximated curve to the original one:","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"fig = Figure()\nax = Axis3(fig[1, 1])\n\nsurface!(x_vec, y_vec, [c([x, y], params(nn_cpu))[1] for x in x_vec, y in y_vec]; alpha = .8, colormap = :darkterrain, transparency = true)\nfig","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"We can also compare the time it takes to train the SymbolicNeuralNetwork to the time it takes to train a standard neural network:","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"loss = FeedForwardLoss()\npb2 = GeometricMachineLearning.ZygotePullback(loss)\no(nn_cpu, dl, batch, n_epochs, pb2.loss, pb2; show_progress = false); # hide\n@time o(nn_cpu, dl, batch, n_epochs, pb2.loss, pb2; show_progress = false);\nnothing # hide","category":"page"},{"location":"symbolic_neural_networks/","page":"Vanilla Symbolic Neural Network","title":"Vanilla Symbolic Neural Network","text":"info: Info\nFor the case presented here we do not observe speed-ups of the symbolic neural network over the standard neural network. For other cases, especially Hamiltonian neural networks, this is however different.","category":"page"},{"location":"hamiltonian_neural_network/#Hamiltonian-Neural-Network","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"","category":"section"},{"location":"hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"Here we build a Hamiltonian neural network as a symbolic neural network.","category":"page"},{"location":"hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"using SymbolicNeuralNetworks\nusing SymbolicNeuralNetworks: params\nusing GeometricMachineLearning\nusing AbstractNeuralNetworks: Dense, UnknownArchitecture, Model\nusing LinearAlgebra: norm\nusing ChainRulesCore\nusing KernelAbstractions\nimport Symbolics\nimport Latexify\n\ninput_dim = 2\nc = Chain(Dense(input_dim, 4, tanh), Dense(4, 1, identity; use_bias = false))\n\nnn = HamiltonianSymbolicNeuralNetwork(c)\nx_hvf = SymbolicNeuralNetworks.vector_field(nn)\nx = x_hvf.x\nhvf = x_hvf.hvf\nhvf |> Latexify.latexify","category":"page"},{"location":"hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"We can now train this Hamiltonian neural network based on vector field data. As a Hamiltonian we take that of a harmonic oscillator:","category":"page"},{"location":"hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"H(z::Array{T}) where T = sum(z.^2) / T(2)\nùïÅ = PoissonTensor(input_dim)\nhvf_analytic(z) = ùïÅ(z)\n\nconst T = Float64\nn_points = 2000\nz_data = randn(T, 2, n_points)\nnothing # hide","category":"page"},{"location":"hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"We now specify a pullback HamiltonianSymbolicNeuralNetwork","category":"page"},{"location":"hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"_pullback = SymbolicPullback(nn)\nnothing # hide","category":"page"},{"location":"hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"We can now train the network:","category":"page"},{"location":"hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"ps = params(NeuralNetwork(c, T))\ndl = DataLoader(z_data, hvf_analytic(z_data))\no = Optimizer(AdamOptimizer(.01), ps)\nbatch = Batch(200)\nconst n_epochs = 150\nnn_dummy = NeuralNetwork(UnknownArchitecture(), c, ps, CPU())\no(nn_dummy, dl, batch, n_epochs, _pullback.loss, _pullback; show_progress = true)\nnothing # hide","category":"page"},{"location":"hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"We now integrate the vector field:","category":"page"},{"location":"hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"using GeometricIntegrators\nhvf_closure(input) = build_nn_function(hvf, x, nn)(input, params(nn_dummy))\nfunction v(v, t, q, params)\n    v .= hvf_closure(q)\nend\npr = ODEProblem(v, (0., 500.), 0.1, [1., 0.])\nsol = integrate(pr, ImplicitMidpoint())","category":"page"},{"location":"hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"using CairoMakie\n\nfig = Figure()\nax = Axis(fig[1, 1])\nlines!(ax, [sol.q[i][1] for i in axes(sol.t, 1)].parent, [sol.q[i][2] for i in axes(sol.t, 1)].parent)","category":"page"},{"location":"hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"We also train a non-Hamiltonian vector field on the same data for comparison:","category":"page"},{"location":"hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"c_nh = Chain(Dense(2, 10, tanh), Dense(10, 4, tanh), Dense(4, 2, identity; use_bias = false))\nnn_nh = NeuralNetwork(c_nh, CPU())\no = Optimizer(AdamOptimizer(T), nn_nh)\no(nn_nh, dl, batch, n_epochs * 10, FeedForwardLoss()) # we train for times as long as before","category":"page"},{"location":"hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"We now integrate the vector field and plot the solution:","category":"page"},{"location":"hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"vf_closure(input) = c_nh(input, nn_nh.params)\nfunction v_nh(v, t, q, params)\n    v .= vf_closure(q)\nend\npr = ODEProblem(v_nh, (0., 500.), 0.1, [1., 0.])\nsol_nh = integrate(pr, ImplicitMidpoint())\n\nlines!(ax, [sol_nh.q[i][1] for i in axes(sol_nh.t, 1)].parent, [sol_nh.q[i][2] for i in axes(sol_nh.t, 1)].parent)","category":"page"},{"location":"double_derivative/#Arbitrarily-Combining-Derivatives","page":"Double Derivative","title":"Arbitrarily Combining Derivatives","text":"","category":"section"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"SymbolicNeuralNetworks can compute derivatives of arbitrary order of a neural network. For this we use two structs:","category":"page"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"SymbolicNeuralNetworks.Jacobian and\nSymbolicNeuralNetworks.Gradient.","category":"page"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"info: Terminology\nWhereas the name Jacobian is standard for the matrix whose entries consist of all partial derivatives of the output of a function, the name Gradient is typically not used the way it is done here. Normally a gradient collects all the partial derivatives of a scalar function. In SymbolicNeuralNetworks the struct Gradient performs all partial derivatives of a symbolic array with respect to all the parameters of a neural network. So if we compute the Gradient of a matrix, then the corresponding routine returns a matrix of neural network parameters, each of which is the standard gradient of a matrix element. So it can be written as:mathttGradientleft( beginpmatrix m_11  m_12  cdots  m_1m  m_21  m_22  cdots  m_2m  vdots  vdots  vdots  vdots  m_n1  m_n2  cdots  m_nm endpmatrix right) = beginpmatrix nabla_mathbbPm_11  nabla_mathbbPm_12  cdots  nabla_mathbbPm_1m  nabla_mathbbPm_21  nabla_mathbbPm_22  cdots  nabla_mathbbPm_2m  vdots  vdots  vdots  vdots  nabla_mathbbPm_n1  nabla_mathbbPm_n2  cdots  nabla_mathbbPm_nm endpmatrixwhere mathbbP are the parameters of the neural network. For computational and consistency reasons each element nabla_mathbbPm_ij are NeuralNetworkParameters.","category":"page"},{"location":"double_derivative/#Jacobian-of-a-Neural-Network","page":"Double Derivative","title":"Jacobian of a Neural Network","text":"","category":"section"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"SymbolicNeuralNetworks.Jacobian differentiates a symbolic expression with respect to the input arguments of a neural network:","category":"page"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"using AbstractNeuralNetworks\nusing SymbolicNeuralNetworks\nusing SymbolicNeuralNetworks: Jacobian, Gradient, derivative\nusing Latexify: latexify\n\nc = Chain(Dense(2, 1, tanh; use_bias = false))\nnn = SymbolicNeuralNetwork(c)\n‚ñ° = Jacobian(nn)\n# we show the derivative with respect to \nderivative(‚ñ°) |> latexify","category":"page"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"Note that the output of nn is one-dimensional and we use the convention","category":"page"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"square_ij = mathrmjacobian_xf_ij = fracpartialpartialx_jf_i","category":"page"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"so the output has shape mathrminput_dimtimesmathrmoutput_dim = 1times2:","category":"page"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"@assert size(derivative(‚ñ°)) == (1, 2) # hide\nsize(derivative(‚ñ°))","category":"page"},{"location":"double_derivative/#Gradient-of-a-Neural-Network","page":"Double Derivative","title":"Gradient of a Neural Network","text":"","category":"section"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"As described above SymbolicNeuralNetworks.Gradient differentiates every element of the array-valued output with respect to the neural network parameters:","category":"page"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"using SymbolicNeuralNetworks: Gradient\n\ng = Gradient(nn)\n\nderivative(g)[1].L1.W |> latexify","category":"page"},{"location":"double_derivative/#Double-Derivatives","page":"Double Derivative","title":"Double Derivatives","text":"","category":"section"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"We can easily differentiate a neural network twice by using SymbolicNeuralNetworks.Jacobian and SymbolicNeuralNetworks.Gradient together. We first use SymbolicNeuralNetworks.Jacobian to differentiate the network output with respect to its input:","category":"page"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"using AbstractNeuralNetworks\nusing SymbolicNeuralNetworks\nusing SymbolicNeuralNetworks: Jacobian, Gradient, derivative, params\nusing Latexify: latexify\n\nc = Chain(Dense(2, 1, tanh))\nnn = SymbolicNeuralNetwork(c)\n‚ñ° = Jacobian(nn)\n# we show the derivative with respect to \nderivative(‚ñ°) |> latexify","category":"page"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"We see that the output is a matrix of size mathrmoutput_dim times mathrminput_dim. We can further compute the gradients of all entries of this matrix with SymbolicNeuralNetworks.Gradient:","category":"page"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"g = Gradient(derivative(‚ñ°), nn)\nnothing # hide","category":"page"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"So SymbolicNeuralNetworks.Gradient differentiates every element of the matrix with respect to all neural network parameters. In order to access the gradient of the first element of the neural network with respect to the weight b in the first layer, we write:","category":"page"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"matrix_index = (1, 1)\nlayer = :L1\nweight = :b\nderivative(g)[matrix_index...][layer][weight] |> latexify","category":"page"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"If we now want to obtain an executable Julia function we have to use build_nn_function. We call this function on:","category":"page"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"x = beginpmatrix 1  0 endpmatrix quad W = beginbmatrix 1  0  0  1 endbmatrix quad b = beginbmatrix 0  0 endbmatrix","category":"page"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"built_function = build_nn_function(derivative(g), params(nn), nn.input)\n\nx = [1., 0.]\nps = NeuralNetworkParameters((L1 = (W = [1. 0.; 0. 1.], b = [0., 0.]), ))\nbuilt_function(x, ps)[matrix_index...][layer][weight]","category":"page"},{"location":"double_derivative/","page":"Double Derivative","title":"Double Derivative","text":"info: Info\nWith SymbolicNeuralNetworks, the structs SymbolicNeuralNetworks.Jacobian, SymbolicNeuralNetworks.Gradient and build_nn_function it is easy to build combinations of derivatives. This is much harder when using Zygote-based AD.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = SymbolicNeuralNetworks","category":"page"},{"location":"#SymbolicNeuralNetworks","page":"Home","title":"SymbolicNeuralNetworks","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SymbolicNeuralNetworks is a library for creating symbolic representations of relatively small neural networks on whose basis more complicated expressions can be build. It should mostly be used together with other packages like GeometricMachineLearning and GeometricIntegrators.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [SymbolicNeuralNetworks]","category":"page"},{"location":"#SymbolicNeuralNetworks.Derivative","page":"Home","title":"SymbolicNeuralNetworks.Derivative","text":"Derivative\n\n\n\n\n\n","category":"type"},{"location":"#SymbolicNeuralNetworks.Gradient","page":"Home","title":"SymbolicNeuralNetworks.Gradient","text":"Gradient <: Derivative\n\nComputes and stores the gradient of a symbolic function with respect to the parameters of a SymbolicNeuralNetwork.\n\nConstructors\n\nGradient(output, nn)\n\nDifferentiate the symbolic output with respect to the parameters of nn.\n\nGradient(nn)\n\nCompute the symbolic output of nn and differentiate it with respect to the parameters of nn.\n\nExamples\n\nusing SymbolicNeuralNetworks: SymbolicNeuralNetwork, Gradient, derivative\nusing AbstractNeuralNetworks\n\nc = Chain(Dense(2, 1, tanh))\nnn = SymbolicNeuralNetwork(c)\n(Gradient(nn) |> derivative)[1].L1.b\n\nImplementation\n\nInternally the constructors are using symbolic_pullback.\n\n\n\n\n\n","category":"type"},{"location":"#SymbolicNeuralNetworks.Jacobian","page":"Home","title":"SymbolicNeuralNetworks.Jacobian","text":"Jacobian <: Derivative\n\nAn instance of Derivative. Computes the derivatives of a neural network with respect to its inputs.\n\nConstructors\n\nJacobian(output, nn)\nJacobian(nn)\n\nCompute the jacobian of a SymbolicNeuralNetwork with respect to the input arguments.\n\nThe output of Jacobian consists of a NamedTuple that has the following keys:\n\na symbolic expression of the input (keyword x),\na symbolic expression of the output (keyword soutput),\na symbolic expression of the gradient (keyword s‚àáoutput).\n\nIf output is not supplied as an input argument than it is taken to be:\n\nsoutput = nn.model(nn.input, params(nn))\n\nImplementation\n\nFor a function fmathbbR^ntomathbbR^m we choose the following convention for the Jacobian:\n\nsquare_ij = fracpartialpartialx_jf_i text ie  square in mathbbR^mtimesn\n\nThis is also used by Zygote and ForwardDiff.\n\nExamples\n\nHere we compute the Jacobian of a single-layer neural network x to mathrmtanh(Wx + b). Its element-wise derivative is:\n\n    fracpartialpartial_isigma(sum_kw_jkx_k + b_j) = sigma(sum_kw_jkx_k + b_j)w_ji\n\nAlso note that for this calculation mathrmtanh(x) = frace^2x - 1e^2x + 1 and mathrmtanh(x) = frac4e^2x(e^2x + 1)^2\n\nWe can use Jacobian together with build_nn_function:\n\nusing SymbolicNeuralNetworks\nusing SymbolicNeuralNetworks: Jacobian, derivative\nusing AbstractNeuralNetworks: Dense, Chain, NeuralNetwork, params\nusing Symbolics\nimport Random\n\nRandom.seed!(123)\n\ninput_dim = 5\noutput_dim = 2\nd = Dense(input_dim, 2, tanh)\nc = Chain(d)\nnn = SymbolicNeuralNetwork(c)\n‚ñ° = SymbolicNeuralNetworks.Jacobian(nn)\n# here we need to access the derivative and convert it into a function\njacobian1 = build_nn_function(derivative(‚ñ°), nn)\nps = params(NeuralNetwork(c, Float64))\ninput = rand(input_dim)\n#derivative\nDtanh(x::Real) = 4 * exp(2 * x) / (1 + exp(2x)) ^ 2\nanalytic_jacobian(i, j) = Dtanh(sum(k -> ps.L1.W[j, k] * input[k], 1:input_dim) + ps.L1.b[j]) * ps.L1.W[j, i]\njacobian1(input, ps) ‚âà [analytic_jacobian(i, j) for j ‚àà 1:output_dim, i ‚àà 1:input_dim]\n\n# output\n\ntrue\n\n\n\n\n\n","category":"type"},{"location":"#SymbolicNeuralNetworks.SymbolicNeuralNetwork","page":"Home","title":"SymbolicNeuralNetworks.SymbolicNeuralNetwork","text":"SymbolicNeuralNetwork <: AbstractSymbolicNeuralNetwork\n\nA symbolic neural network realizes a symbolic represenation (of small neural networks).\n\nFields\n\nThe struct has the following fields:\n\narchitecture: the neural network architecture,\nmodel: the model (typically a Chain that is the realization of the architecture),\nparams: the symbolic parameters of the network.\nsinput: the symbolic input of the network.\n\nConstructors\n\nSymbolicNeuralNetwork(nn)\n\nMake a SymbolicNeuralNetwork based on a AbstractNeuralNetworks.Network.\n\n\n\n\n\n","category":"type"},{"location":"#SymbolicNeuralNetworks.SymbolicPullback","page":"Home","title":"SymbolicNeuralNetworks.SymbolicPullback","text":"SymbolicPullback <: AbstractPullback\n\nSymbolicPullback computes the symbolic pullback of a loss function.\n\nExamples\n\nusing SymbolicNeuralNetworks\nusing AbstractNeuralNetworks\nusing AbstractNeuralNetworks: params\nimport Random\nRandom.seed!(123)\n\nc = Chain(Dense(2, 1, tanh))\nnn = NeuralNetwork(c)\nsnn = SymbolicNeuralNetwork(nn)\nloss = FeedForwardLoss()\npb = SymbolicPullback(snn, loss)\nps = params(nn)\ntypeof(pb(ps, nn.model, (rand(2), rand(1)))[2](1))\n\n# output\n\n@NamedTuple{L1::@NamedTuple{W::Matrix{Float64}, b::Vector{Float64}}}\n\nImplementation\n\nAn instance of SymbolicPullback stores\n\nloss: an instance of a NetworkLoss,\nfun: a function that is used to compute the pullback.\n\nIf we call the functor of an instance of SymbolicPullback on model, ps and input it returns:\n\n_pullback.loss(model, ps, input...), _pullback.fun(input..., ps)\n\nwhere the second output argument is again a function.\n\nExtended help\n\nWe note the following seeming peculiarity:\n\nusing SymbolicNeuralNetworks\nusing AbstractNeuralNetworks: Chain, Dense, NeuralNetwork, FeedForwardLoss, params\nusing Symbolics\nimport Random\nRandom.seed!(123)\n\nc = Chain(Dense(2, 1, tanh))\nnn = NeuralNetwork(c)\nsnn = SymbolicNeuralNetwork(nn)\nloss = FeedForwardLoss()\npb = SymbolicPullback(snn, loss)\ninput_output = (rand(2), rand(1))\nloss_and_pullback = pb(params(nn), nn.model, input_output)\n# note that we apply the second argument to another input `1`\npb_values = loss_and_pullback[2](1)\n\n@variables soutput[1:SymbolicNeuralNetworks.output_dimension(nn.model)]\nsymbolic_pullbacks = SymbolicNeuralNetworks.symbolic_pullback(loss(nn.model, params(snn), snn.input, soutput), snn)\npb_values2 = build_nn_function(symbolic_pullbacks, params(snn), snn.input, soutput)(input_output[1], input_output[2], params(nn))\n\npb_values == (pb_values2 |> SymbolicNeuralNetworks._get_contents |> SymbolicNeuralNetworks._get_params)\n\n# output\n\ntrue\n\nSee the docstrings for symbolic_pullback, build_nn_function, _get_params and _get_contents for more info on the functions that we used here. The noteworthy thing in the expression above is that the functor of SymbolicPullback returns two objects: the first one is the loss value evaluated for the relevant parameters and inputs. The second one is a function that takes again an input argument and then finally returns the partial derivatives. But why do we need this extra step with another function?\n\ninfo: Reverse Accumulation\nIn machine learning we typically do reverse accumulation to perform automatic differentiation (AD). Assuming we are given a function that is the composition of simpler functions f = f_1circf_2circcdotscircf_nmathbbR^ntomathbbR^m reverse differentiation starts with output sensitivities and then successively feeds them through f_n, f_n-1 etc. So it does:(nabla_xf)^T = (nabla_xf_1)^T(nabla_f_1(x)f_2)^Tcdots(nabla_f_n-1(cdotsx)f_n)^T(do)where doinmathbbR^m are the output sensitivities and the jacobians are stepwise multiplied from the left. So we propagate from the output stepwise back to the input. If we have m=1, i.e. if the output is one-dimensional, then the output sensitivities may simply be taken to be do = 1.\n\nSo in theory we could leave out this extra step: returning an object (that is stored in pb.fun) can be seen as unnecessary as we could simply store the equivalent of pb.fun(1.) in an instance of SymbolicPullback. It is however customary for a pullback to return a callable function (that depends on the output sensitivities), which is why we also choose to do this here, even if the output sensitivities are a scalar quantity.\n\n\n\n\n\n","category":"type"},{"location":"#SymbolicNeuralNetworks._build_nn_function-Tuple{Union{Symbolics.Num, AbstractArray{Symbolics.Num}, AbstractArray{<:SymbolicUtils.BasicSymbolic}}, AbstractNeuralNetworks.NeuralNetworkParameters, Symbolics.Arr, Symbolics.Arr}","page":"Home","title":"SymbolicNeuralNetworks._build_nn_function","text":"_build_nn_function(eq, params, sinput, soutput)\n\nBuild a function that can process a matrix. See build_nn_function(::EqT, ::NeuralNetworkParameters, ::Symbolics.Arr).\n\nImplementation\n\nNote that we have two input arguments here which means this method processes code differently than _build_nn_function(::EqT, ::NeuralNetworkParameters, ::Symbolics.Arr, ::Symbolics.Arr). Here we call:\n\nfix_create_array,\nrewrite_arguments2,\nmodify_input_arguments2,\nfix_map_reduce.\n\nSee the docstrings for those functions for details on how the code is modified. \n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks._build_nn_function-Tuple{Union{Symbolics.Num, AbstractArray{Symbolics.Num}, AbstractArray{<:SymbolicUtils.BasicSymbolic}}, AbstractNeuralNetworks.NeuralNetworkParameters, Symbolics.Arr}","page":"Home","title":"SymbolicNeuralNetworks._build_nn_function","text":"_build_nn_function(eq, params, sinput)\n\nBuild a function that can process a matrix. This is used as a starting point for build_nn_function.\n\nExamples\n\nusing SymbolicNeuralNetworks: _build_nn_function, SymbolicNeuralNetwork\nusing AbstractNeuralNetworks: params, Chain, Dense, NeuralNetwork\nimport Random\nRandom.seed!(123)\n\nc = Chain(Dense(2, 1, tanh))\nnn = NeuralNetwork(c)\nsnn = SymbolicNeuralNetwork(nn)\neq = c(snn.input, params(snn))\nbuilt_function = _build_nn_function(eq, params(snn), snn.input)\nbuilt_function([1. 2.; 3. 4.], params(nn), 1)\n\n# output\n\n1-element Vector{Float64}:\n 0.9912108161055604\n\nNote that we have to supply an extra argument (index) to _build_nn_function that we do not have to supply to build_nn_function.\n\nImplementation\n\nThis first calls Symbolics.build_function with the keyword argument expression = Val{true} and then modifies the generated code by calling:\n\nfix_create_array,\nrewrite_arguments,\nmodify_input_arguments,\nfix_map_reduce.\n\nSee the docstrings for those functions for details on how the code is modified. \n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks._get_contents-Tuple{NamedTuple}","page":"Home","title":"SymbolicNeuralNetworks._get_contents","text":"_get_contents(nt::AbstractArray{<:NamedTuple})\n\nReturn the contents of a one-dimensional vector.\n\nExamples\n\nusing SymbolicNeuralNetworks: _get_contents\n\n_get_contents([(a = \"element_contained_in_vector\", )])\n\n# output\n\n(a = \"element_contained_in_vector\",)\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks._get_params-Tuple{NamedTuple}","page":"Home","title":"SymbolicNeuralNetworks._get_params","text":"_get_params(ps::NeuralNetworkParameters)\n\nReturn the NamedTuple that's equivalent to the NeuralNetworkParameters.\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks._modify_integer-Tuple{AbstractString}","page":"Home","title":"SymbolicNeuralNetworks._modify_integer","text":"_modify_integer\n\nIf the input is a single integer, subtract 1 from it.\n\nExamples\n\nusing SymbolicNeuralNetworks: _modify_integer\n\ns = [\"2\", \"hello\", \"hello2\", \"3\"]\n_modify_integer.(s)\n\n# output\n4-element Vector{String}:\n \"1\"\n \"hello\"\n \"hello2\"\n \"2\"\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks._modify_integer2-Tuple{AbstractString}","page":"Home","title":"SymbolicNeuralNetworks._modify_integer2","text":"_modify_integer2\n\nIf the input is a single integer, subtract 2 from it.\n\nExamples\n\nusing SymbolicNeuralNetworks: _modify_integer2\n\ns = [\"3\", \"hello\", \"hello2\", \"4\"]\n_modify_integer2.(s)\n\n# output\n4-element Vector{String}:\n \"1\"\n \"hello\"\n \"hello2\"\n \"2\"\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.apply_element_wise-Tuple{AbstractArray, AbstractNeuralNetworks.NeuralNetworkParameters, Vararg{AbstractArray}}","page":"Home","title":"SymbolicNeuralNetworks.apply_element_wise","text":"apply_element_wise(ps, params, input...)\n\nApply a function element-wise. ps is an Array where each entry of the array is are NeuralNetworkParameters that store functions. See apply_element_wise(::NeuralNetworkParameters, ::NeuralNetworkParameters, ::Any).\n\nExamples\n\nVector: \n\nusing SymbolicNeuralNetworks: apply_element_wise\nusing AbstractNeuralNetworks: NeuralNetworkParameters\n\n# parameter values\nparams = NeuralNetworkParameters((a = 1., b = 2.))\nps = [NeuralNetworkParameters((val1 = (input, params) -> input .+ params.a, val2 = (input, params) -> input .+ params.b))]\napply_element_wise(ps, params, [1.])\n\n# output\n\n1-element Vector{NeuralNetworkParameters{(:val1, :val2), Tuple{Vector{Float64}, Vector{Float64}}}}:\n NeuralNetworkParameters{(:val1, :val2), Tuple{Vector{Float64}, Vector{Float64}}}((val1 = [2.0], val2 = [3.0]))\n\nMatrix: \n\nusing SymbolicNeuralNetworks: apply_element_wise\nusing AbstractNeuralNetworks: NeuralNetworkParameters\n\n# parameter values\nparams = NeuralNetworkParameters((a = 1., b = 2.))\nsc_ps = NeuralNetworkParameters((val1 = (input, params) -> input .+ params.a, val2 = (input, params) -> input .+ params.b))\nps = [sc_ps sc_ps]\napply_element_wise(ps, params, [1.]) |> typeof\n\n# output\n\nMatrix{NeuralNetworkParameters{(:val1, :val2), Tuple{Vector{Float64}, Vector{Float64}}}} (alias for Array{NeuralNetworkParameters{(:val1, :val2), Tuple{Array{Float64, 1}, Array{Float64, 1}}}, 2})\n\nImplementation\n\nThis is generating a @generated function.\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.apply_element_wise-Tuple{AbstractNeuralNetworks.NeuralNetworkParameters, AbstractNeuralNetworks.NeuralNetworkParameters, Any}","page":"Home","title":"SymbolicNeuralNetworks.apply_element_wise","text":"apply_element_wise(ps, params, input...)\n\nApply a function element-wise. ps is a NeuralNetworkParameters-valued function.\n\nExamples\n\nusing SymbolicNeuralNetworks: apply_element_wise\nusing AbstractNeuralNetworks: NeuralNetworkParameters\n\n# parameter values\nparams = NeuralNetworkParameters((a = 1., b = 2.))\nps = NeuralNetworkParameters((val1 = (input, params) -> input + params.a, val2 = (input, params) -> input + params.b))\napply_element_wise(ps, params, 1.)\n\n# output\n\nNeuralNetworkParameters{(:val1, :val2), Tuple{Float64, Float64}}((val1 = 2.0, val2 = 3.0))\n\nImplementation\n\nThis is generating a @generated function.\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.build_nn_function-Tuple{AbstractArray{<:Union{AbstractNeuralNetworks.NeuralNetworkParameters, NamedTuple}}, AbstractNeuralNetworks.NeuralNetworkParameters, Vararg{Symbolics.Arr}}","page":"Home","title":"SymbolicNeuralNetworks.build_nn_function","text":"build_nn_function(eqs::AbstractArray{<:NeuralNetworkParameters}, sparams, sinput...)\n\nBuild an executable function based on an array of symbolic equations eqs.\n\nExamples\n\nusing SymbolicNeuralNetworks: build_nn_function, SymbolicNeuralNetwork\nusing AbstractNeuralNetworks: Chain, Dense, NeuralNetwork, params\nimport Random\nRandom.seed!(123)\n\nch = Chain(Dense(2, 1, tanh))\nnn = NeuralNetwork(ch)\nsnn = SymbolicNeuralNetwork(nn)\neqs = [(a = ch(snn.input, params(snn)), b = ch(snn.input, params(snn)).^2), (c = ch(snn.input, params(snn)).^3, )]\nfuncs = build_nn_function(eqs, params(snn), snn.input)\ninput = [1., 2.]\nfuncs_evaluated = funcs(input, params(nn))\n\n# output\n\n2-element Vector{NamedTuple}:\n (a = [0.985678060655224], b = [0.9715612392570434])\n (c = [0.9576465981186686],)\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.build_nn_function-Tuple{Any, AbstractSymbolicNeuralNetwork, Any}","page":"Home","title":"SymbolicNeuralNetworks.build_nn_function","text":"build_nn_function(eqs, nn, soutput)\n\nBuild an executable function that can also depend on an output. It is then called with:\n\nbuilt_function(input, output, ps)\n\nAlso compare this to build_nn_function(::EqT, ::AbstractSymbolicNeuralNetwork).\n\nExtended Help\n\nSee the extended help section of build_nn_function(::EqT, ::AbstractSymbolicNeuralNetwork).\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.build_nn_function-Tuple{Union{AbstractNeuralNetworks.NeuralNetworkParameters, NamedTuple}, AbstractNeuralNetworks.NeuralNetworkParameters, Vararg{Symbolics.Arr}}","page":"Home","title":"SymbolicNeuralNetworks.build_nn_function","text":"build_nn_function(eqs::Union{NamedTuple, NeuralNetworkParameters}, sparams, sinput...)\n\nReturn a function that takes an input, (optionally) an output and neural network parameters and returns a NeuralNetworkParameters-valued output.\n\nExamples\n\nusing SymbolicNeuralNetworks: build_nn_function, SymbolicNeuralNetwork\nusing AbstractNeuralNetworks: Chain, Dense, NeuralNetwork, params\nimport Random\nRandom.seed!(123)\n\nc = Chain(Dense(2, 1, tanh))\nnn = NeuralNetwork(c)\nsnn = SymbolicNeuralNetwork(nn)\neqs = (a = c(snn.input, params(snn)), b = c(snn.input, params(snn)).^2)\nfuncs = build_nn_function(eqs, params(snn), snn.input)\ninput = [1., 2.]\nfuncs_evaluated = funcs(input, params(nn))\n\n# output\n\n(a = [0.985678060655224], b = [0.9715612392570434])\n\nImplementation\n\nInternally this is using function_valued_parameters and apply_element_wise.\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.build_nn_function-Tuple{Union{Symbolics.Num, AbstractArray{Symbolics.Num}, AbstractArray{<:SymbolicUtils.BasicSymbolic}}, AbstractSymbolicNeuralNetwork}","page":"Home","title":"SymbolicNeuralNetworks.build_nn_function","text":"build_nn_function(eq, nn)\n\nBuild an executable function based on a symbolic equation, a symbolic input array and a SymbolicNeuralNetwork.\n\nThis function can be called with:\n\nbuilt_function(input, ps)\n\nImplementation\n\nInternally this is calling _build_nn_function and then parallelizing the expression via the index k.\n\nExtended Help\n\nThe functions mentioned in the implementation section were adjusted ad-hoc to deal with problems that emerged on the fly.  Other problems may occur. In case you bump into one please open an issue on github.\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.derivative-Tuple{SymbolicNeuralNetworks.Gradient}","page":"Home","title":"SymbolicNeuralNetworks.derivative","text":"derivative(g)\n\nExamples\n\nusing SymbolicNeuralNetworks: SymbolicNeuralNetwork, Gradient, derivative, symbolic_pullback\nusing AbstractNeuralNetworks\n\nc = Chain(Dense(2, 1, tanh))\nnn = SymbolicNeuralNetwork(c)\ng = Gradient(nn)\n‚àá = derivative(g)\n\nisequal(‚àá, symbolic_pullback(g.output, nn))\n\n# output\n\ntrue\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.fix_create_array-Tuple{AbstractString}","page":"Home","title":"SymbolicNeuralNetworks.fix_create_array","text":"fixcreatearray(s)\n\nFix a problem that occurs in connection with create_array.\n\nThe function create_array from SymbolicUtils.Code takes as first input the type of a symbolic array.  For reasons that are not entirely clear yet the first argument of create_array ends up being Àç‚Çãarg2, which is a NamedTuple of symoblic arrays. We solve this problem by replacing typeof(Àç‚Çãarg[0-9]+) with Array, which seems to be the most generic possible input to create_array.\n\nExamples\n\nusing SymbolicNeuralNetworks: fix_create_array\n\ns = \"(SymbolicUtils.Code.create_array)(typeof(Àç‚Çãarg2)\"\nfix_create_array(s)\n\n# output\n\n\"SymbolicUtils.Code.create_array(typeof(sinput)\"\n\nImplementation\n\nThis is used for _build_nn_function(::EqT, ::NeuralNetworkParameters, ::Symbolics.Arr) and _build_nn_function(::EqT, ::NeuralNetworkParameters, ::Symbolics.Arr, ::Symbolics.Arr).\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.fix_map_reduce-Tuple{AbstractString}","page":"Home","title":"SymbolicNeuralNetworks.fix_map_reduce","text":"fix_map_reduce(s)\n\nReplace Symbolics._mapreduce with mapreduce (from Base).\n\nWhen we generate a function with Symbolics.build_function it often contains Symbolics._mapreduce which cannot be differentiated with Zygote.  We get around this by replacing Symbolics._mapreduce with mapreduce and also doing:\n\nreplace(s, \", Colon(), (:init => false,)\" => \", dims = Colon()\")\n\nImplementation\n\nThis is used for _build_nn_function(::EqT, ::NeuralNetworkParameters, ::Symbolics.Arr) and _build_nn_function(::EqT, ::NeuralNetworkParameters, ::Symbolics.Arr, ::Symbolics.Arr).\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.function_valued_parameters-Tuple{AbstractNeuralNetworks.NeuralNetworkParameters, AbstractNeuralNetworks.NeuralNetworkParameters, Vararg{Symbolics.Arr}}","page":"Home","title":"SymbolicNeuralNetworks.function_valued_parameters","text":"function_valued_parameters(eqs::Union{NamedTuple, NeuralNetworkParameters}, sparams, sinput...)\n\nReturn an executable function for each entry in eqs. This still has to be processed with apply_element_wise.\n\nExamples\n\nusing SymbolicNeuralNetworks: function_valued_parameters, SymbolicNeuralNetwork\nusing AbstractNeuralNetworks: Chain, Dense, NeuralNetwork, params\nimport Random\nRandom.seed!(123)\n\nc = Chain(Dense(2, 1, tanh))\nnn = NeuralNetwork(c)\nsnn = SymbolicNeuralNetwork(nn)\neqs = (a = c(snn.input, params(snn)), b = c(snn.input, params(snn)).^2)\nfuncs = function_valued_parameters(eqs, params(snn), snn.input)\ninput = [1., 2.]\nps = params(nn)\na = c(input, ps)\nb = c(input, ps).^2\n\n(funcs.a(input, ps), funcs.b(input, ps)) .‚âà (a, b)\n\n# output\n\n(true, true)\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.make_kernel-Tuple{AbstractString}","page":"Home","title":"SymbolicNeuralNetworks.make_kernel","text":"Examples\n\nusing SymbolicNeuralNetworks\n\ns = \"function (sinput, ps)\\n begin\\n getindex(sinput, 1) + getindex(sinput, 2) \\n end\\n end\"\nSymbolicNeuralNetworks.make_kernel(s)\n\n# output\n\n\"function (sinput, ps, k)\\n begin\\n getindex(sinput, 1, k) + getindex(sinput, 2, k) \\n end\\n end\"\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.make_kernel2-Tuple{AbstractString}","page":"Home","title":"SymbolicNeuralNetworks.make_kernel2","text":"Examples\n\nusing SymbolicNeuralNetworks\n\ns = \"function (sinput, soutput, ps)\\n begin\\n getindex(sinput, 1) + getindex(soutput, 2) \\n end\\n end\"\nSymbolicNeuralNetworks.make_kernel2(s)\n\n# output\n\n\"function (sinput, soutput, ps, k)\\n begin\\n getindex(sinput, 1, k) + getindex(soutput, 2, k) \\n end\\n end\"\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.modify_input_arguments-Tuple{AbstractString}","page":"Home","title":"SymbolicNeuralNetworks.modify_input_arguments","text":"modify_input_arguments(s)\n\nChange input arguments of type (sinput, ps.L1, ps.L2) etc to (sinput, ps). This should be used after rewrite_arguments. Also see build_nn_function.\n\nExamples\n\nusing SymbolicNeuralNetworks: modify_input_arguments\n\ns = \"(sinput, ps.L1, ps.L2, ps.L3)\"\nmodify_input_arguments(s)\n\n# output\n\"(sinput, ps)\"\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.modify_input_arguments2-Tuple{AbstractString}","page":"Home","title":"SymbolicNeuralNetworks.modify_input_arguments2","text":"modify_input_arguments2(s)\n\nChange input arguments of type (sinput, soutput, ps.L1, ps.L2) etc to (sinput, soutput, ps). This should be used after rewrite_arguments.\n\nExamples\n\nusing SymbolicNeuralNetworks: modify_input_arguments2\n\ns = \"(sinput, soutput, ps.L1, ps.L2, ps.L3)\"\nmodify_input_arguments2(s)\n\n# output\n\"(sinput, soutput, ps)\"\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.rewrite_arguments-Tuple{AbstractString}","page":"Home","title":"SymbolicNeuralNetworks.rewrite_arguments","text":"rewrite_arguments(s)\n\nReplace Àç‚Çãarg2, Àç‚Çãarg3, ... with ps.L1, ps.L2 etc. This is used after Symbolics.build_function.\n\nExamples\n\nusing SymbolicNeuralNetworks: rewrite_arguments\ns = \"We test if strings that contain Àç‚Çãarg2 and Àç‚Çãarg3 can be converted in the right way.\"\nrewrite_arguments(s)\n\n# output\n\"We test if strings that contain ps.L1 and ps.L2 can be converted in the right way.\"\n\nImplementation\n\nThe input is first split at the relevant points and then we call _modify_integer. The routine _modify_integer ensures that we start counting at 1 and not at 2. By defaut the arguments of the generated function that we get after applying Symbolics.build_function are (x, Àç‚Çãarg2, Àç‚Çãarg3) etc. We first change this to (x, ps.L2, ps.L3) etc. and then to (x, ps.L1, ps.L2) etc. via _modify_integer.\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.rewrite_arguments2-Tuple{AbstractString}","page":"Home","title":"SymbolicNeuralNetworks.rewrite_arguments2","text":"rewrite_arguments2(s)\n\nReplace Àç‚Çãarg3, Àç‚Çãarg4, ... with ps.L1, ps.L2 etc. Note that we subtract two from the input, unlike rewrite_arguments where it is one.\n\nExamples\n\nusing SymbolicNeuralNetworks: rewrite_arguments2\ns = \"We test if strings that contain Àç‚Çãarg3 and Àç‚Çãarg4 can be converted in the right way.\"\nrewrite_arguments2(s)\n\n# output\n\"We test if strings that contain ps.L1 and ps.L2 can be converted in the right way.\"\n\nImplementation\n\nThe input is first split at the relevant points and then we call _modify_integer2. The routine _modify_integer2 ensures that we start counting at 1 and not at 3. See rewrite_arguments.\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.symbolic_pullback-Tuple{Union{Symbolics.Num, AbstractArray{Symbolics.Num}, AbstractArray{<:SymbolicUtils.BasicSymbolic}}, AbstractSymbolicNeuralNetwork}","page":"Home","title":"SymbolicNeuralNetworks.symbolic_pullback","text":"symbolic_pullback(nn, output)\n\nThis takes a symbolic output that depends on the parameters in nn and returns the corresponding pullback (a symbolic expression).\n\nThis is used by Gradient and SymbolicPullback.\n\nExamples\n\nusing SymbolicNeuralNetworks: SymbolicNeuralNetwork, symbolic_pullback\nusing AbstractNeuralNetworks\nusing AbstractNeuralNetworks: params\nusing LinearAlgebra: norm\n\nc = Chain(Dense(2, 1, tanh))\nnn = SymbolicNeuralNetwork(c)\noutput = c(nn.input, params(nn))\nspb = symbolic_pullback(output, nn)\n\nspb[1].L1.b\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.symboliccounter!-Tuple{Dict, Symbol}","page":"Home","title":"SymbolicNeuralNetworks.symboliccounter!","text":"symboliccounter!(cache, arg; redundancy)\n\nAdd a specific argument to the cache.\n\nExamples\n\nusing SymbolicNeuralNetworks: symboliccounter!\n\ncache = Dict()\nvar = symboliccounter!(cache, :var)\n(cache, var)\n\n# output\n(Dict{Any, Any}(:var => 1), :var_1)\n\n\n\n\n\n\n","category":"method"},{"location":"#SymbolicNeuralNetworks.symbolize!","page":"Home","title":"SymbolicNeuralNetworks.symbolize!","text":"symbolize!(cache, nt, var_name)\n\nSymbolize all the arguments in nt.\n\nExamples\n\nusing SymbolicNeuralNetworks: symbolize!\n\ncache = Dict()\nsym = symbolize!(cache, .1, :X)\n(sym, cache)\n\n# output\n\n(X_1, Dict{Any, Any}(:X => 1))\n\nusing SymbolicNeuralNetworks: symbolize!\n\ncache = Dict()\narr = rand(2, 1)\nsym_scalar = symbolize!(cache, .1, :X)\nsym_array = symbolize!(cache, arr, :Y)\n(sym_array, cache)\n\n# output\n\n(Y_1[Base.OneTo(2),Base.OneTo(1)], Dict{Any, Any}(:X => 1, :Y => 1))\n\nNote that the for the second case the cache is storing a scalar under :X and an array under :Y. If we use the same label for both we get:\n\nusing SymbolicNeuralNetworks: symbolize!\n\ncache = Dict()\narr = rand(2, 1)\nsym_scalar = symbolize!(cache, .1, :X)\nsym_array = symbolize!(cache, arr, :X)\n(sym_array, cache)\n\n# output\n\n(X_2[Base.OneTo(2),Base.OneTo(1)], Dict{Any, Any}(:X => 2))\n\nWe can also use symbolize! with NamedTuples:\n\nusing SymbolicNeuralNetworks: symbolize!\n\ncache = Dict()\nnt = (a = 1, b = [1, 2])\nsym = symbolize!(cache, nt, :X)\n(sym, cache)\n\n# output\n\n((a = X_1, b = X_2[Base.OneTo(2)]), Dict{Any, Any}(:X => 2))\n\nAnd for neural network parameters:\n\nusing SymbolicNeuralNetworks: symbolize!\nusing AbstractNeuralNetworks: NeuralNetwork, params, Chain, Dense\n\nnn = NeuralNetwork(Chain(Dense(1, 2; use_bias = false), Dense(2, 1; use_bias = false)))\ncache = Dict()\nsym = symbolize!(cache, params(nn), :X) |> typeof\n\n# output\n\nAbstractNeuralNetworks.NeuralNetworkParameters{(:L1, :L2), Tuple{@NamedTuple{W::Symbolics.Arr{Symbolics.Num, 2}}, @NamedTuple{W::Symbolics.Arr{Symbolics.Num, 2}}}}\n\nImplementation\n\nInternally this is using symboliccounter!. This function is also adjusting/altering the cache (that is optionally supplied as an input argument).\n\n\n\n\n\n","category":"function"}]
}
